{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import gc\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.frame_l = 32 # the length of frames\n",
    "        self.joint_n = 22 # the number of joints\n",
    "        self.joint_d = 3 # the dimension of joints\n",
    "        self.clc_coarse = 14 # the number of coarse class\n",
    "        self.clc_fine = 28 # the number of fine-grained class\n",
    "        self.feat_d = 231\n",
    "        self.filters = 16\n",
    "        self.data_dir = '/mnt/nasbi/homes/fan/projects/action/skeleton/data/SHREC/'\n",
    "C = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poses_diff(x):\n",
    "    H, W = x.get_shape()[1],x.get_shape()[2]\n",
    "    x = tf.subtract(x[:,1:,...],x[:,:-1,...])\n",
    "    x = tf.image.resize_nearest_neighbor(x,size=[H.value,W.value],align_corners=False) # should not alignment here\n",
    "    return x\n",
    "\n",
    "def pose_motion(P,frame_l):\n",
    "    P_diff_slow = Lambda(lambda x: poses_diff(x))(P)\n",
    "    P_diff_slow = Reshape((frame_l,-1))(P_diff_slow)\n",
    "    P_fast = Lambda(lambda x: x[:,::2,...])(P)\n",
    "    P_diff_fast = Lambda(lambda x: poses_diff(x))(P_fast)\n",
    "    P_diff_fast = Reshape((int(frame_l/2),-1))(P_diff_fast)\n",
    "    return P_diff_slow,P_diff_fast\n",
    "    \n",
    "def c1D(x,filters,kernel):\n",
    "    x = Conv1D(filters, kernel_size=kernel,padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def block(x,filters):\n",
    "    x = c1D(x,filters,3)\n",
    "    x = c1D(x,filters,3)\n",
    "    return x\n",
    "    \n",
    "def d1D(x,filters):\n",
    "    x = Dense(filters,use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def build_FM(frame_l=32,joint_n=22,joint_d=2,feat_d=231,filters=16):   \n",
    "    M = Input(shape=(frame_l,feat_d))\n",
    "    P = Input(shape=(frame_l,joint_n,joint_d))\n",
    "    \n",
    "    diff_slow,diff_fast = pose_motion(P,frame_l)\n",
    "    \n",
    "    x = c1D(M,filters*2,1)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = c1D(x,filters,3)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = c1D(x,filters,1)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "\n",
    "    x_d_slow = c1D(diff_slow,filters*2,1)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "    x_d_slow = c1D(x_d_slow,filters,3)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "    x_d_slow = c1D(x_d_slow,filters,1)\n",
    "    x_d_slow = MaxPool1D(2)(x_d_slow)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "        \n",
    "    x_d_fast = c1D(diff_fast,filters*2,1)\n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "    x_d_fast = c1D(x_d_fast,filters,3) \n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "    x_d_fast = c1D(x_d_fast,filters,1) \n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "   \n",
    "    x = concatenate([x,x_d_slow,x_d_fast])\n",
    "    x = block(x,filters*2)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = block(x,filters*4)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "\n",
    "    x = block(x,filters*8)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    return Model(inputs=[M,P],outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_DD_Net(frame_l=32,joint_n=22,joint_d=3,feat_d=66,clc_num=14,filters=16):\n",
    "    M = Input(name='M', shape=(frame_l,feat_d))  \n",
    "    P = Input(name='P', shape=(frame_l,joint_n,joint_d)) \n",
    "    \n",
    "    FM = build_FM(frame_l,joint_n,joint_d,feat_d,filters)\n",
    "    \n",
    "    x = FM([M,P])\n",
    "\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(clc_num, activation='softmax')(x)\n",
    "    \n",
    "    ######################Self-supervised part\n",
    "    model = Model(inputs=[M,P],outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DD_Net = build_DD_Net(C.frame_l,C.joint_n,C.joint_d,C.feat_d,C.clc_fine,C.filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "M (InputLayer)                  (None, 32, 231)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "P (InputLayer)                  (None, 32, 22, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4, 128)       119392      M[0][0]                          \n",
      "                                                                 P[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16384       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 128)          0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16384       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 128)          0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 28)           3612        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 156,796\n",
      "Trainable params: 155,004\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DD_Net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DD_Net.load_weights('weights/fine_lite.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train = pickle.load(open(C.data_dir+\"train.pkl\", \"rb\"))\n",
    "Test = pickle.load(open(C.data_dir+\"test.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without frame_sampling train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = []\n",
    "X_1 = []\n",
    "Y = []\n",
    "for i in tqdm(range(len(Train['pose']))): \n",
    "    p = np.copy(Train['pose'][i]).reshape([-1,22,3])\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_fine)\n",
    "    label[Train['fine_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_0.append(M)\n",
    "    X_1.append(p)\n",
    "    Y.append(label)\n",
    "\n",
    "X_0 = np.stack(X_0)  \n",
    "X_1 = np.stack(X_1) \n",
    "Y = np.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_0 = []\n",
    "X_test_1 = []\n",
    "Y_test = []\n",
    "for i in tqdm(range(len(Test['pose']))): \n",
    "    p = np.copy(Test['pose'][i]).reshape([-1,22,3])\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_fine)\n",
    "    label[Test['fine_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_test_0.append(M)\n",
    "    X_test_1.append(p)\n",
    "    Y_test.append(label)\n",
    "\n",
    "X_test_0 = np.stack(X_test_0) \n",
    "X_test_1 = np.stack(X_test_1)  \n",
    "Y_test = np.stack(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/400\n",
      "1960/1960 [==============================] - 9s 5ms/step - loss: 3.9400 - acc: 0.0342 - val_loss: 3.3576 - val_acc: 0.0429\n",
      "Epoch 2/400\n",
      "1960/1960 [==============================] - 1s 319us/step - loss: 3.8103 - acc: 0.0357 - val_loss: 3.2570 - val_acc: 0.0833\n",
      "Epoch 3/400\n",
      "1960/1960 [==============================] - 1s 264us/step - loss: 3.6932 - acc: 0.0510 - val_loss: 3.1817 - val_acc: 0.0976\n",
      "Epoch 4/400\n",
      "1960/1960 [==============================] - 1s 313us/step - loss: 3.5799 - acc: 0.0500 - val_loss: 3.1183 - val_acc: 0.1167\n",
      "Epoch 5/400\n",
      "1960/1960 [==============================] - 0s 254us/step - loss: 3.4485 - acc: 0.0653 - val_loss: 3.0569 - val_acc: 0.1333\n",
      "Epoch 6/400\n",
      "1960/1960 [==============================] - 1s 302us/step - loss: 3.3921 - acc: 0.0770 - val_loss: 3.0071 - val_acc: 0.1393\n",
      "Epoch 7/400\n",
      "1960/1960 [==============================] - 1s 270us/step - loss: 3.2802 - acc: 0.0949 - val_loss: 2.9483 - val_acc: 0.1536\n",
      "Epoch 8/400\n",
      "1960/1960 [==============================] - 1s 283us/step - loss: 3.2470 - acc: 0.0949 - val_loss: 2.8863 - val_acc: 0.1690\n",
      "Epoch 9/400\n",
      "1960/1960 [==============================] - 0s 240us/step - loss: 3.2003 - acc: 0.0990 - val_loss: 2.8314 - val_acc: 0.1774\n",
      "Epoch 10/400\n",
      "1960/1960 [==============================] - 1s 300us/step - loss: 3.1206 - acc: 0.1224 - val_loss: 2.7945 - val_acc: 0.1869\n",
      "Epoch 11/400\n",
      "1960/1960 [==============================] - 1s 269us/step - loss: 3.0563 - acc: 0.1250 - val_loss: 2.7610 - val_acc: 0.1893\n",
      "Epoch 12/400\n",
      "1960/1960 [==============================] - 1s 292us/step - loss: 2.9492 - acc: 0.1485 - val_loss: 2.7246 - val_acc: 0.2000\n",
      "Epoch 13/400\n",
      "1960/1960 [==============================] - 0s 252us/step - loss: 2.9524 - acc: 0.1469 - val_loss: 2.6646 - val_acc: 0.2226\n",
      "Epoch 14/400\n",
      "1960/1960 [==============================] - 1s 286us/step - loss: 2.8850 - acc: 0.1709 - val_loss: 2.5957 - val_acc: 0.2298\n",
      "Epoch 15/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 2.8591 - acc: 0.1653 - val_loss: 2.5196 - val_acc: 0.2429\n",
      "Epoch 16/400\n",
      "1960/1960 [==============================] - 1s 282us/step - loss: 2.7503 - acc: 0.1980 - val_loss: 2.4575 - val_acc: 0.2464\n",
      "Epoch 17/400\n",
      "1960/1960 [==============================] - 1s 273us/step - loss: 2.7062 - acc: 0.1969 - val_loss: 2.3833 - val_acc: 0.2643\n",
      "Epoch 18/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 2.6175 - acc: 0.2321 - val_loss: 2.3051 - val_acc: 0.2833\n",
      "Epoch 19/400\n",
      "1960/1960 [==============================] - 0s 243us/step - loss: 2.5928 - acc: 0.2327 - val_loss: 2.2359 - val_acc: 0.3012\n",
      "Epoch 20/400\n",
      "1960/1960 [==============================] - 1s 275us/step - loss: 2.5279 - acc: 0.2500 - val_loss: 2.1709 - val_acc: 0.3202\n",
      "Epoch 21/400\n",
      "1960/1960 [==============================] - 0s 237us/step - loss: 2.4883 - acc: 0.2515 - val_loss: 2.1169 - val_acc: 0.3310\n",
      "Epoch 22/400\n",
      "1960/1960 [==============================] - 1s 295us/step - loss: 2.4152 - acc: 0.2903 - val_loss: 2.0708 - val_acc: 0.3464\n",
      "Epoch 23/400\n",
      "1960/1960 [==============================] - 1s 268us/step - loss: 2.3709 - acc: 0.3082 - val_loss: 2.0316 - val_acc: 0.3619\n",
      "Epoch 24/400\n",
      "1960/1960 [==============================] - 1s 296us/step - loss: 2.3357 - acc: 0.3102 - val_loss: 1.9998 - val_acc: 0.3667\n",
      "Epoch 25/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 2.2343 - acc: 0.3342 - val_loss: 1.9847 - val_acc: 0.3702\n",
      "Epoch 26/400\n",
      "1960/1960 [==============================] - 1s 291us/step - loss: 2.2347 - acc: 0.3490 - val_loss: 1.9683 - val_acc: 0.3702\n",
      "Epoch 27/400\n",
      "1960/1960 [==============================] - 1s 279us/step - loss: 2.1816 - acc: 0.3536 - val_loss: 1.9576 - val_acc: 0.3631\n",
      "Epoch 28/400\n",
      "1960/1960 [==============================] - 1s 278us/step - loss: 2.1534 - acc: 0.3469 - val_loss: 1.9503 - val_acc: 0.3548\n",
      "Epoch 29/400\n",
      "1960/1960 [==============================] - 1s 283us/step - loss: 2.0796 - acc: 0.3908 - val_loss: 1.9463 - val_acc: 0.3571\n",
      "Epoch 30/400\n",
      "1960/1960 [==============================] - 1s 290us/step - loss: 2.0260 - acc: 0.4041 - val_loss: 1.9464 - val_acc: 0.3571\n",
      "Epoch 31/400\n",
      "1960/1960 [==============================] - 0s 253us/step - loss: 2.0152 - acc: 0.4020 - val_loss: 1.9610 - val_acc: 0.3560\n",
      "Epoch 32/400\n",
      "1960/1960 [==============================] - 1s 282us/step - loss: 1.9445 - acc: 0.4245 - val_loss: 1.9591 - val_acc: 0.3702\n",
      "Epoch 33/400\n",
      "1960/1960 [==============================] - 1s 260us/step - loss: 1.9134 - acc: 0.4276 - val_loss: 1.9590 - val_acc: 0.3607\n",
      "Epoch 34/400\n",
      "1960/1960 [==============================] - 1s 299us/step - loss: 1.8992 - acc: 0.4276 - val_loss: 1.9654 - val_acc: 0.3702\n",
      "Epoch 35/400\n",
      "1960/1960 [==============================] - 0s 251us/step - loss: 1.8485 - acc: 0.4474 - val_loss: 1.9520 - val_acc: 0.3833\n",
      "Epoch 36/400\n",
      "1960/1960 [==============================] - 1s 269us/step - loss: 1.8193 - acc: 0.4474 - val_loss: 1.9280 - val_acc: 0.3905\n",
      "Epoch 37/400\n",
      "1960/1960 [==============================] - 0s 249us/step - loss: 1.7413 - acc: 0.4857 - val_loss: 1.8945 - val_acc: 0.3845\n",
      "Epoch 38/400\n",
      "1960/1960 [==============================] - 1s 306us/step - loss: 1.7186 - acc: 0.5010 - val_loss: 1.8612 - val_acc: 0.3929\n",
      "Epoch 39/400\n",
      "1960/1960 [==============================] - 1s 297us/step - loss: 1.6871 - acc: 0.5245 - val_loss: 1.8030 - val_acc: 0.4095\n",
      "Epoch 40/400\n",
      "1960/1960 [==============================] - 1s 325us/step - loss: 1.6635 - acc: 0.5097 - val_loss: 1.7510 - val_acc: 0.4357\n",
      "Epoch 41/400\n",
      "1960/1960 [==============================] - 1s 259us/step - loss: 1.6185 - acc: 0.5194 - val_loss: 1.7046 - val_acc: 0.4512\n",
      "Epoch 42/400\n",
      "1960/1960 [==============================] - 1s 275us/step - loss: 1.5570 - acc: 0.5352 - val_loss: 1.6638 - val_acc: 0.4619\n",
      "Epoch 43/400\n",
      "1960/1960 [==============================] - 1s 275us/step - loss: 1.5334 - acc: 0.5459 - val_loss: 1.6433 - val_acc: 0.4702\n",
      "Epoch 44/400\n",
      "1960/1960 [==============================] - 1s 306us/step - loss: 1.5236 - acc: 0.5587 - val_loss: 1.6461 - val_acc: 0.4560\n",
      "Epoch 45/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 1.4800 - acc: 0.5776 - val_loss: 1.6697 - val_acc: 0.4464\n",
      "Epoch 46/400\n",
      "1960/1960 [==============================] - 1s 309us/step - loss: 1.4450 - acc: 0.5714 - val_loss: 1.6924 - val_acc: 0.4345\n",
      "Epoch 47/400\n",
      "1960/1960 [==============================] - 1s 269us/step - loss: 1.4142 - acc: 0.5770 - val_loss: 1.7440 - val_acc: 0.4167\n",
      "Epoch 48/400\n",
      "1960/1960 [==============================] - 1s 278us/step - loss: 1.3788 - acc: 0.5995 - val_loss: 1.8097 - val_acc: 0.4071\n",
      "Epoch 49/400\n",
      "1960/1960 [==============================] - 1s 267us/step - loss: 1.3612 - acc: 0.5867 - val_loss: 1.8499 - val_acc: 0.3952\n",
      "Epoch 50/400\n",
      "1960/1960 [==============================] - 1s 296us/step - loss: 1.3047 - acc: 0.6184 - val_loss: 1.8660 - val_acc: 0.3905\n",
      "Epoch 51/400\n",
      "1960/1960 [==============================] - 1s 260us/step - loss: 1.2525 - acc: 0.6434 - val_loss: 1.8215 - val_acc: 0.4000\n",
      "Epoch 52/400\n",
      "1960/1960 [==============================] - 1s 279us/step - loss: 1.2453 - acc: 0.6454 - val_loss: 1.7461 - val_acc: 0.4095\n",
      "Epoch 53/400\n",
      "1960/1960 [==============================] - 1s 275us/step - loss: 1.2415 - acc: 0.6311 - val_loss: 1.6718 - val_acc: 0.4286\n",
      "Epoch 54/400\n",
      "1960/1960 [==============================] - 1s 281us/step - loss: 1.2167 - acc: 0.6398 - val_loss: 1.6120 - val_acc: 0.4440\n",
      "Epoch 55/400\n",
      "1960/1960 [==============================] - 0s 253us/step - loss: 1.1985 - acc: 0.6577 - val_loss: 1.5777 - val_acc: 0.4524\n",
      "Epoch 56/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 1.1714 - acc: 0.6500 - val_loss: 1.5757 - val_acc: 0.4560\n",
      "Epoch 57/400\n",
      "1960/1960 [==============================] - 1s 266us/step - loss: 1.1496 - acc: 0.6673 - val_loss: 1.6153 - val_acc: 0.4536\n",
      "Epoch 58/400\n",
      "1960/1960 [==============================] - 1s 285us/step - loss: 1.1012 - acc: 0.6781 - val_loss: 1.6626 - val_acc: 0.4476\n",
      "Epoch 59/400\n",
      "1960/1960 [==============================] - 1s 271us/step - loss: 1.0933 - acc: 0.6740 - val_loss: 1.7231 - val_acc: 0.4357\n",
      "Epoch 60/400\n",
      "1960/1960 [==============================] - 1s 311us/step - loss: 1.0598 - acc: 0.6816 - val_loss: 1.7304 - val_acc: 0.4369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/400\n",
      "1960/1960 [==============================] - 0s 251us/step - loss: 1.0686 - acc: 0.6796 - val_loss: 1.7142 - val_acc: 0.4405\n",
      "Epoch 62/400\n",
      "1960/1960 [==============================] - 1s 284us/step - loss: 1.0383 - acc: 0.7036 - val_loss: 1.6931 - val_acc: 0.4476\n",
      "Epoch 63/400\n",
      "1960/1960 [==============================] - 1s 278us/step - loss: 1.0092 - acc: 0.6995 - val_loss: 1.6592 - val_acc: 0.4548\n",
      "Epoch 64/400\n",
      "1960/1960 [==============================] - 1s 321us/step - loss: 1.0260 - acc: 0.6964 - val_loss: 1.6120 - val_acc: 0.4643\n",
      "Epoch 65/400\n",
      "1960/1960 [==============================] - 1s 299us/step - loss: 1.0054 - acc: 0.6908 - val_loss: 1.5622 - val_acc: 0.4833\n",
      "Epoch 66/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.9344 - acc: 0.7235 - val_loss: 1.4810 - val_acc: 0.5095\n",
      "Epoch 67/400\n",
      "1960/1960 [==============================] - 1s 260us/step - loss: 0.9611 - acc: 0.7153 - val_loss: 1.4158 - val_acc: 0.5310\n",
      "Epoch 68/400\n",
      "1960/1960 [==============================] - 1s 294us/step - loss: 0.9324 - acc: 0.7270 - val_loss: 1.3694 - val_acc: 0.5321\n",
      "Epoch 69/400\n",
      "1960/1960 [==============================] - 0s 248us/step - loss: 0.9336 - acc: 0.7276 - val_loss: 1.3347 - val_acc: 0.5393\n",
      "Epoch 70/400\n",
      "1960/1960 [==============================] - 1s 291us/step - loss: 0.8875 - acc: 0.7367 - val_loss: 1.2576 - val_acc: 0.5607\n",
      "Epoch 71/400\n",
      "1960/1960 [==============================] - 1s 283us/step - loss: 0.8808 - acc: 0.7316 - val_loss: 1.1932 - val_acc: 0.5917\n",
      "Epoch 72/400\n",
      "1960/1960 [==============================] - 1s 289us/step - loss: 0.8838 - acc: 0.7454 - val_loss: 1.1591 - val_acc: 0.6083\n",
      "Epoch 73/400\n",
      "1960/1960 [==============================] - 1s 274us/step - loss: 0.8515 - acc: 0.7454 - val_loss: 1.1410 - val_acc: 0.6107\n",
      "Epoch 74/400\n",
      "1960/1960 [==============================] - 1s 308us/step - loss: 0.8345 - acc: 0.7429 - val_loss: 1.1602 - val_acc: 0.5976\n",
      "Epoch 75/400\n",
      "1960/1960 [==============================] - 1s 280us/step - loss: 0.8295 - acc: 0.7531 - val_loss: 1.1817 - val_acc: 0.5929\n",
      "Epoch 76/400\n",
      "1960/1960 [==============================] - 1s 303us/step - loss: 0.8415 - acc: 0.7520 - val_loss: 1.2137 - val_acc: 0.5810\n",
      "Epoch 77/400\n",
      "1960/1960 [==============================] - 0s 249us/step - loss: 0.8245 - acc: 0.7556 - val_loss: 1.2330 - val_acc: 0.5726\n",
      "Epoch 78/400\n",
      "1960/1960 [==============================] - 1s 317us/step - loss: 0.7966 - acc: 0.7653 - val_loss: 1.2640 - val_acc: 0.5619\n",
      "Epoch 79/400\n",
      "1960/1960 [==============================] - 1s 266us/step - loss: 0.7733 - acc: 0.7786 - val_loss: 1.3317 - val_acc: 0.5381\n",
      "Epoch 80/400\n",
      "1960/1960 [==============================] - 1s 261us/step - loss: 0.7429 - acc: 0.7944 - val_loss: 1.3857 - val_acc: 0.5226\n",
      "Epoch 81/400\n",
      "1960/1960 [==============================] - 1s 292us/step - loss: 0.7476 - acc: 0.7842 - val_loss: 1.4390 - val_acc: 0.5071\n",
      "Epoch 82/400\n",
      "1960/1960 [==============================] - 1s 306us/step - loss: 0.7479 - acc: 0.7827 - val_loss: 1.4343 - val_acc: 0.5083\n",
      "Epoch 83/400\n",
      "1960/1960 [==============================] - 1s 287us/step - loss: 0.7475 - acc: 0.7821 - val_loss: 1.4309 - val_acc: 0.5095\n",
      "Epoch 84/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 0.7397 - acc: 0.7806 - val_loss: 1.3760 - val_acc: 0.5262\n",
      "Epoch 85/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 0.7161 - acc: 0.7934 - val_loss: 1.3094 - val_acc: 0.5512\n",
      "Epoch 86/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 0.7039 - acc: 0.7969 - val_loss: 1.2944 - val_acc: 0.5536\n",
      "Epoch 87/400\n",
      "1960/1960 [==============================] - 1s 258us/step - loss: 0.6981 - acc: 0.7934 - val_loss: 1.3166 - val_acc: 0.5429\n",
      "Epoch 88/400\n",
      "1960/1960 [==============================] - 1s 291us/step - loss: 0.6808 - acc: 0.8015 - val_loss: 1.3777 - val_acc: 0.5238\n",
      "Epoch 89/400\n",
      "1960/1960 [==============================] - 1s 266us/step - loss: 0.6758 - acc: 0.8005 - val_loss: 1.3500 - val_acc: 0.5286\n",
      "Epoch 90/400\n",
      "1960/1960 [==============================] - 1s 295us/step - loss: 0.6727 - acc: 0.8051 - val_loss: 1.2708 - val_acc: 0.5571\n",
      "Epoch 91/400\n",
      "1960/1960 [==============================] - 1s 289us/step - loss: 0.6670 - acc: 0.8036 - val_loss: 1.2314 - val_acc: 0.5679\n",
      "Epoch 92/400\n",
      "1960/1960 [==============================] - 1s 309us/step - loss: 0.6468 - acc: 0.8107 - val_loss: 1.2062 - val_acc: 0.5798\n",
      "Epoch 93/400\n",
      "1960/1960 [==============================] - 1s 297us/step - loss: 0.6571 - acc: 0.8138 - val_loss: 1.1944 - val_acc: 0.5833\n",
      "Epoch 94/400\n",
      "1960/1960 [==============================] - 1s 288us/step - loss: 0.6374 - acc: 0.8092 - val_loss: 1.1925 - val_acc: 0.5869\n",
      "Epoch 95/400\n",
      "1960/1960 [==============================] - 1s 260us/step - loss: 0.6249 - acc: 0.8245 - val_loss: 1.1454 - val_acc: 0.5976\n",
      "Epoch 96/400\n",
      "1960/1960 [==============================] - 1s 316us/step - loss: 0.6081 - acc: 0.8240 - val_loss: 1.1084 - val_acc: 0.6190\n",
      "Epoch 97/400\n",
      "1960/1960 [==============================] - 1s 282us/step - loss: 0.6162 - acc: 0.8286 - val_loss: 1.0623 - val_acc: 0.6369\n",
      "Epoch 98/400\n",
      "1960/1960 [==============================] - 1s 311us/step - loss: 0.5827 - acc: 0.8327 - val_loss: 0.9848 - val_acc: 0.6690\n",
      "Epoch 99/400\n",
      "1960/1960 [==============================] - 0s 249us/step - loss: 0.5965 - acc: 0.8270 - val_loss: 0.9459 - val_acc: 0.6881\n",
      "Epoch 100/400\n",
      "1960/1960 [==============================] - 1s 286us/step - loss: 0.5643 - acc: 0.8510 - val_loss: 0.9612 - val_acc: 0.6798\n",
      "Epoch 101/400\n",
      "1960/1960 [==============================] - 1s 269us/step - loss: 0.5764 - acc: 0.8286 - val_loss: 0.9651 - val_acc: 0.6798\n",
      "Epoch 102/400\n",
      "1960/1960 [==============================] - 1s 303us/step - loss: 0.5704 - acc: 0.8388 - val_loss: 0.9726 - val_acc: 0.6762\n",
      "Epoch 103/400\n",
      "1960/1960 [==============================] - 1s 286us/step - loss: 0.5635 - acc: 0.8347 - val_loss: 0.9940 - val_acc: 0.6655\n",
      "Epoch 104/400\n",
      "1960/1960 [==============================] - 1s 281us/step - loss: 0.5559 - acc: 0.8372 - val_loss: 1.0221 - val_acc: 0.6488\n",
      "Epoch 105/400\n",
      "1960/1960 [==============================] - 1s 300us/step - loss: 0.5361 - acc: 0.8505 - val_loss: 1.0671 - val_acc: 0.6333\n",
      "Epoch 106/400\n",
      "1960/1960 [==============================] - 1s 315us/step - loss: 0.5521 - acc: 0.8352 - val_loss: 1.1147 - val_acc: 0.6167\n",
      "Epoch 107/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.5356 - acc: 0.8474 - val_loss: 1.1367 - val_acc: 0.6119\n",
      "Epoch 108/400\n",
      "1960/1960 [==============================] - 1s 309us/step - loss: 0.5400 - acc: 0.8429 - val_loss: 1.1607 - val_acc: 0.6107\n",
      "Epoch 109/400\n",
      "1960/1960 [==============================] - 1s 264us/step - loss: 0.5291 - acc: 0.8485 - val_loss: 1.2265 - val_acc: 0.5869\n",
      "Epoch 110/400\n",
      "1960/1960 [==============================] - 1s 286us/step - loss: 0.4986 - acc: 0.8633 - val_loss: 1.2544 - val_acc: 0.5774\n",
      "Epoch 111/400\n",
      "1960/1960 [==============================] - 1s 279us/step - loss: 0.4895 - acc: 0.8510 - val_loss: 1.2509 - val_acc: 0.5869\n",
      "Epoch 112/400\n",
      "1960/1960 [==============================] - 1s 299us/step - loss: 0.4870 - acc: 0.8622 - val_loss: 1.1930 - val_acc: 0.6119\n",
      "Epoch 113/400\n",
      "1960/1960 [==============================] - 1s 281us/step - loss: 0.5050 - acc: 0.8577 - val_loss: 1.1311 - val_acc: 0.6357\n",
      "Epoch 114/400\n",
      "1960/1960 [==============================] - 1s 296us/step - loss: 0.4789 - acc: 0.8699 - val_loss: 1.0593 - val_acc: 0.6512\n",
      "Epoch 115/400\n",
      "1960/1960 [==============================] - 1s 261us/step - loss: 0.4793 - acc: 0.8704 - val_loss: 0.9899 - val_acc: 0.6738\n",
      "Epoch 116/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 0.4819 - acc: 0.8633 - val_loss: 0.9041 - val_acc: 0.6929\n",
      "Epoch 117/400\n",
      "1960/1960 [==============================] - 1s 268us/step - loss: 0.4579 - acc: 0.8714 - val_loss: 0.8430 - val_acc: 0.7155\n",
      "Epoch 118/400\n",
      "1960/1960 [==============================] - 1s 309us/step - loss: 0.4348 - acc: 0.8765 - val_loss: 0.8050 - val_acc: 0.7321\n",
      "Epoch 119/400\n",
      "1960/1960 [==============================] - 1s 287us/step - loss: 0.4553 - acc: 0.8770 - val_loss: 0.8056 - val_acc: 0.7310\n",
      "Epoch 120/400\n",
      "1960/1960 [==============================] - 1s 257us/step - loss: 0.4512 - acc: 0.8709 - val_loss: 0.8459 - val_acc: 0.7131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/400\n",
      "1960/1960 [==============================] - 0s 248us/step - loss: 0.4623 - acc: 0.8658 - val_loss: 0.9671 - val_acc: 0.6702\n",
      "Epoch 122/400\n",
      "1960/1960 [==============================] - 1s 268us/step - loss: 0.4289 - acc: 0.8867 - val_loss: 1.1013 - val_acc: 0.6286\n",
      "Epoch 123/400\n",
      "1960/1960 [==============================] - 1s 258us/step - loss: 0.4289 - acc: 0.8821 - val_loss: 1.2643 - val_acc: 0.5774\n",
      "Epoch 124/400\n",
      "1960/1960 [==============================] - 1s 312us/step - loss: 0.4320 - acc: 0.8847 - val_loss: 1.3816 - val_acc: 0.5476\n",
      "Epoch 125/400\n",
      "1960/1960 [==============================] - 1s 278us/step - loss: 0.4132 - acc: 0.8903 - val_loss: 1.4683 - val_acc: 0.5345\n",
      "Epoch 126/400\n",
      "1960/1960 [==============================] - 1s 274us/step - loss: 0.4234 - acc: 0.8796 - val_loss: 1.4547 - val_acc: 0.5429\n",
      "Epoch 127/400\n",
      "1960/1960 [==============================] - 0s 253us/step - loss: 0.4057 - acc: 0.8781 - val_loss: 1.3556 - val_acc: 0.5655\n",
      "Epoch 128/400\n",
      "1960/1960 [==============================] - 1s 272us/step - loss: 0.4222 - acc: 0.8801 - val_loss: 1.2685 - val_acc: 0.5940\n",
      "Epoch 129/400\n",
      "1960/1960 [==============================] - 0s 252us/step - loss: 0.3948 - acc: 0.8923 - val_loss: 1.1918 - val_acc: 0.6167\n",
      "Epoch 130/400\n",
      "1960/1960 [==============================] - 0s 244us/step - loss: 0.4057 - acc: 0.8872 - val_loss: 1.1428 - val_acc: 0.6190\n",
      "Epoch 131/400\n",
      "1960/1960 [==============================] - 0s 247us/step - loss: 0.3832 - acc: 0.8923 - val_loss: 1.0858 - val_acc: 0.6345\n",
      "Epoch 132/400\n",
      "1960/1960 [==============================] - 1s 274us/step - loss: 0.3857 - acc: 0.8893 - val_loss: 1.0404 - val_acc: 0.6536\n",
      "Epoch 133/400\n",
      "1960/1960 [==============================] - 0s 249us/step - loss: 0.3746 - acc: 0.8878 - val_loss: 0.9808 - val_acc: 0.6750\n",
      "Epoch 134/400\n",
      "1960/1960 [==============================] - 1s 285us/step - loss: 0.3680 - acc: 0.8964 - val_loss: 0.9625 - val_acc: 0.6869\n",
      "Epoch 135/400\n",
      "1960/1960 [==============================] - 1s 256us/step - loss: 0.3730 - acc: 0.8959 - val_loss: 0.9395 - val_acc: 0.6940\n",
      "Epoch 136/400\n",
      "1960/1960 [==============================] - 1s 291us/step - loss: 0.3806 - acc: 0.8923 - val_loss: 0.9827 - val_acc: 0.6845\n",
      "Epoch 137/400\n",
      "1960/1960 [==============================] - 1s 258us/step - loss: 0.3675 - acc: 0.8959 - val_loss: 1.0920 - val_acc: 0.6488\n",
      "Epoch 138/400\n",
      "1960/1960 [==============================] - 1s 289us/step - loss: 0.3662 - acc: 0.8964 - val_loss: 1.1381 - val_acc: 0.6345\n",
      "Epoch 139/400\n",
      "1960/1960 [==============================] - 0s 254us/step - loss: 0.3423 - acc: 0.9046 - val_loss: 1.0957 - val_acc: 0.6548\n",
      "Epoch 140/400\n",
      "1960/1960 [==============================] - 1s 275us/step - loss: 0.3397 - acc: 0.9015 - val_loss: 1.0219 - val_acc: 0.6738\n",
      "Epoch 141/400\n",
      "1960/1960 [==============================] - 1s 274us/step - loss: 0.3372 - acc: 0.9056 - val_loss: 0.9366 - val_acc: 0.7012\n",
      "Epoch 142/400\n",
      "1960/1960 [==============================] - 1s 269us/step - loss: 0.3351 - acc: 0.9148 - val_loss: 0.8852 - val_acc: 0.7179\n",
      "Epoch 143/400\n",
      "1960/1960 [==============================] - 1s 258us/step - loss: 0.3444 - acc: 0.8995 - val_loss: 0.8382 - val_acc: 0.7298\n",
      "Epoch 144/400\n",
      "1960/1960 [==============================] - 1s 296us/step - loss: 0.3435 - acc: 0.9082 - val_loss: 0.8697 - val_acc: 0.7190\n",
      "Epoch 145/400\n",
      "1960/1960 [==============================] - 1s 275us/step - loss: 0.3274 - acc: 0.9056 - val_loss: 0.9808 - val_acc: 0.6679\n",
      "Epoch 146/400\n",
      "1960/1960 [==============================] - 1s 280us/step - loss: 0.3299 - acc: 0.9153 - val_loss: 1.0400 - val_acc: 0.6440\n",
      "Epoch 147/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 0.3192 - acc: 0.9138 - val_loss: 1.0548 - val_acc: 0.6417\n",
      "Epoch 148/400\n",
      "1960/1960 [==============================] - 1s 267us/step - loss: 0.3125 - acc: 0.9148 - val_loss: 1.0579 - val_acc: 0.6429\n",
      "Epoch 149/400\n",
      "1960/1960 [==============================] - 1s 272us/step - loss: 0.3195 - acc: 0.9107 - val_loss: 1.0423 - val_acc: 0.6464\n",
      "Epoch 150/400\n",
      "1960/1960 [==============================] - 1s 261us/step - loss: 0.3467 - acc: 0.9000 - val_loss: 0.9008 - val_acc: 0.6964\n",
      "Epoch 151/400\n",
      "1960/1960 [==============================] - 1s 376us/step - loss: 0.3310 - acc: 0.9082 - val_loss: 0.7673 - val_acc: 0.7524\n",
      "Epoch 152/400\n",
      "1960/1960 [==============================] - 1s 382us/step - loss: 0.2984 - acc: 0.9122 - val_loss: 0.6966 - val_acc: 0.7786\n",
      "Epoch 153/400\n",
      "1960/1960 [==============================] - 1s 392us/step - loss: 0.3028 - acc: 0.9102 - val_loss: 0.6588 - val_acc: 0.7929\n",
      "Epoch 154/400\n",
      "1960/1960 [==============================] - 1s 379us/step - loss: 0.2787 - acc: 0.9189 - val_loss: 0.6538 - val_acc: 0.7976\n",
      "Epoch 155/400\n",
      "1960/1960 [==============================] - 1s 378us/step - loss: 0.2921 - acc: 0.9143 - val_loss: 0.6496 - val_acc: 0.8024\n",
      "Epoch 156/400\n",
      "1960/1960 [==============================] - 1s 357us/step - loss: 0.2958 - acc: 0.9199 - val_loss: 0.6788 - val_acc: 0.7964\n",
      "Epoch 157/400\n",
      "1960/1960 [==============================] - 1s 282us/step - loss: 0.2825 - acc: 0.9199 - val_loss: 0.7528 - val_acc: 0.7702\n",
      "Epoch 158/400\n",
      "1960/1960 [==============================] - 1s 292us/step - loss: 0.3000 - acc: 0.9087 - val_loss: 0.8632 - val_acc: 0.7333\n",
      "Epoch 159/400\n",
      "1960/1960 [==============================] - 1s 279us/step - loss: 0.2917 - acc: 0.9204 - val_loss: 1.0294 - val_acc: 0.6655\n",
      "Epoch 160/400\n",
      "1960/1960 [==============================] - 1s 324us/step - loss: 0.3077 - acc: 0.9138 - val_loss: 1.1272 - val_acc: 0.6310\n",
      "Epoch 161/400\n",
      "1960/1960 [==============================] - 0s 219us/step - loss: 0.2556 - acc: 0.9306 - val_loss: 1.1290 - val_acc: 0.6226\n",
      "Epoch 162/400\n",
      "1960/1960 [==============================] - 1s 282us/step - loss: 0.2708 - acc: 0.9255 - val_loss: 1.1284 - val_acc: 0.6214\n",
      "Epoch 163/400\n",
      "1960/1960 [==============================] - 0s 196us/step - loss: 0.3011 - acc: 0.9189 - val_loss: 1.0208 - val_acc: 0.6631\n",
      "Epoch 164/400\n",
      "1960/1960 [==============================] - 1s 302us/step - loss: 0.2761 - acc: 0.9260 - val_loss: 0.9243 - val_acc: 0.6917\n",
      "Epoch 165/400\n",
      "1960/1960 [==============================] - 0s 225us/step - loss: 0.2718 - acc: 0.9240 - val_loss: 0.8139 - val_acc: 0.7298\n",
      "Epoch 166/400\n",
      "1960/1960 [==============================] - 1s 356us/step - loss: 0.2630 - acc: 0.9327 - val_loss: 0.6543 - val_acc: 0.7964\n",
      "Epoch 167/400\n",
      "1960/1960 [==============================] - 0s 251us/step - loss: 0.2604 - acc: 0.9230 - val_loss: 0.5700 - val_acc: 0.8262\n",
      "Epoch 168/400\n",
      "1960/1960 [==============================] - 1s 321us/step - loss: 0.2715 - acc: 0.9250 - val_loss: 0.5331 - val_acc: 0.8333\n",
      "Epoch 169/400\n",
      "1960/1960 [==============================] - 0s 243us/step - loss: 0.2547 - acc: 0.9311 - val_loss: 0.5163 - val_acc: 0.8417\n",
      "Epoch 170/400\n",
      "1960/1960 [==============================] - 1s 289us/step - loss: 0.2596 - acc: 0.9245 - val_loss: 0.5038 - val_acc: 0.8488\n",
      "Epoch 171/400\n",
      "1960/1960 [==============================] - 0s 214us/step - loss: 0.2566 - acc: 0.9230 - val_loss: 0.5058 - val_acc: 0.8440\n",
      "Epoch 172/400\n",
      "1960/1960 [==============================] - 1s 299us/step - loss: 0.2554 - acc: 0.9276 - val_loss: 0.5134 - val_acc: 0.8440\n",
      "Epoch 173/400\n",
      "1960/1960 [==============================] - 0s 224us/step - loss: 0.2558 - acc: 0.9291 - val_loss: 0.5282 - val_acc: 0.8357\n",
      "Epoch 174/400\n",
      "1960/1960 [==============================] - 1s 293us/step - loss: 0.2459 - acc: 0.9327 - val_loss: 0.5434 - val_acc: 0.8286\n",
      "Epoch 175/400\n",
      "1960/1960 [==============================] - 0s 227us/step - loss: 0.2741 - acc: 0.9209 - val_loss: 0.5611 - val_acc: 0.8202\n",
      "Epoch 176/400\n",
      "1960/1960 [==============================] - 1s 352us/step - loss: 0.2617 - acc: 0.9255 - val_loss: 0.5863 - val_acc: 0.8083\n",
      "Epoch 177/400\n",
      "1960/1960 [==============================] - 0s 248us/step - loss: 0.2521 - acc: 0.9332 - val_loss: 0.6334 - val_acc: 0.7857\n",
      "Epoch 178/400\n",
      "1960/1960 [==============================] - 1s 345us/step - loss: 0.2602 - acc: 0.9311 - val_loss: 0.6936 - val_acc: 0.7631\n",
      "Epoch 179/400\n",
      "1960/1960 [==============================] - 0s 220us/step - loss: 0.2515 - acc: 0.9311 - val_loss: 0.7938 - val_acc: 0.7333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/400\n",
      "1960/1960 [==============================] - 1s 305us/step - loss: 0.2486 - acc: 0.9327 - val_loss: 0.8636 - val_acc: 0.7095\n",
      "Epoch 181/400\n",
      "1960/1960 [==============================] - 0s 237us/step - loss: 0.2494 - acc: 0.9265 - val_loss: 0.9170 - val_acc: 0.6869\n",
      "Epoch 182/400\n",
      "1960/1960 [==============================] - 1s 296us/step - loss: 0.2352 - acc: 0.9342 - val_loss: 0.8984 - val_acc: 0.6976\n",
      "Epoch 183/400\n",
      "1960/1960 [==============================] - 0s 238us/step - loss: 0.2149 - acc: 0.9393 - val_loss: 0.8095 - val_acc: 0.7286\n",
      "Epoch 184/400\n",
      "1960/1960 [==============================] - 1s 289us/step - loss: 0.2538 - acc: 0.9265 - val_loss: 0.6801 - val_acc: 0.7786\n",
      "Epoch 185/400\n",
      "1960/1960 [==============================] - 0s 225us/step - loss: 0.2342 - acc: 0.9393 - val_loss: 0.6016 - val_acc: 0.8071\n",
      "Epoch 186/400\n",
      "1960/1960 [==============================] - 1s 299us/step - loss: 0.2446 - acc: 0.9296 - val_loss: 0.5528 - val_acc: 0.8321\n",
      "Epoch 187/400\n",
      "1960/1960 [==============================] - 0s 224us/step - loss: 0.2297 - acc: 0.9372 - val_loss: 0.5215 - val_acc: 0.8429\n",
      "Epoch 188/400\n",
      "1960/1960 [==============================] - 1s 285us/step - loss: 0.2178 - acc: 0.9383 - val_loss: 0.5038 - val_acc: 0.8524\n",
      "Epoch 189/400\n",
      "1960/1960 [==============================] - 0s 205us/step - loss: 0.2320 - acc: 0.9296 - val_loss: 0.4968 - val_acc: 0.8548\n",
      "Epoch 190/400\n",
      "1960/1960 [==============================] - 1s 269us/step - loss: 0.2351 - acc: 0.9398 - val_loss: 0.5027 - val_acc: 0.8464\n",
      "Epoch 191/400\n",
      "1960/1960 [==============================] - 0s 201us/step - loss: 0.2483 - acc: 0.9352 - val_loss: 0.5104 - val_acc: 0.8476\n",
      "Epoch 192/400\n",
      "1960/1960 [==============================] - 1s 319us/step - loss: 0.2377 - acc: 0.9316 - val_loss: 0.5129 - val_acc: 0.8393\n",
      "Epoch 193/400\n",
      "1960/1960 [==============================] - 0s 214us/step - loss: 0.2255 - acc: 0.9393 - val_loss: 0.5120 - val_acc: 0.8405\n",
      "Epoch 194/400\n",
      "1960/1960 [==============================] - 1s 293us/step - loss: 0.2319 - acc: 0.9296 - val_loss: 0.5181 - val_acc: 0.8417\n",
      "Epoch 195/400\n",
      "1960/1960 [==============================] - 0s 216us/step - loss: 0.2381 - acc: 0.9332 - val_loss: 0.5221 - val_acc: 0.8429\n",
      "Epoch 196/400\n",
      "1960/1960 [==============================] - 1s 287us/step - loss: 0.2324 - acc: 0.9398 - val_loss: 0.5177 - val_acc: 0.8417\n",
      "Epoch 197/400\n",
      "1960/1960 [==============================] - 0s 233us/step - loss: 0.2101 - acc: 0.9413 - val_loss: 0.5218 - val_acc: 0.8417\n",
      "Epoch 198/400\n",
      "1960/1960 [==============================] - 1s 306us/step - loss: 0.2268 - acc: 0.9383 - val_loss: 0.5264 - val_acc: 0.8393\n",
      "Epoch 199/400\n",
      "1960/1960 [==============================] - 0s 229us/step - loss: 0.2264 - acc: 0.9352 - val_loss: 0.5205 - val_acc: 0.8417\n",
      "Epoch 200/400\n",
      "1960/1960 [==============================] - 1s 301us/step - loss: 0.2104 - acc: 0.9444 - val_loss: 0.5065 - val_acc: 0.8500\n",
      "Epoch 201/400\n",
      "1960/1960 [==============================] - 0s 235us/step - loss: 0.2334 - acc: 0.9316 - val_loss: 0.4863 - val_acc: 0.8560\n",
      "Epoch 202/400\n",
      "1960/1960 [==============================] - 1s 295us/step - loss: 0.2259 - acc: 0.9321 - val_loss: 0.4622 - val_acc: 0.8643\n",
      "Epoch 203/400\n",
      "1960/1960 [==============================] - 0s 207us/step - loss: 0.2192 - acc: 0.9449 - val_loss: 0.4462 - val_acc: 0.8702\n",
      "Epoch 204/400\n",
      "1960/1960 [==============================] - 1s 322us/step - loss: 0.2187 - acc: 0.9393 - val_loss: 0.4364 - val_acc: 0.8738\n",
      "Epoch 205/400\n",
      "1960/1960 [==============================] - 0s 219us/step - loss: 0.2322 - acc: 0.9352 - val_loss: 0.4293 - val_acc: 0.8774\n",
      "Epoch 206/400\n",
      "1960/1960 [==============================] - 1s 322us/step - loss: 0.2358 - acc: 0.9367 - val_loss: 0.4242 - val_acc: 0.8810\n",
      "Epoch 207/400\n",
      "1960/1960 [==============================] - 0s 219us/step - loss: 0.2265 - acc: 0.9372 - val_loss: 0.4206 - val_acc: 0.8845\n",
      "Epoch 208/400\n",
      "1960/1960 [==============================] - 1s 297us/step - loss: 0.2096 - acc: 0.9439 - val_loss: 0.4183 - val_acc: 0.8857\n",
      "Epoch 209/400\n",
      "1960/1960 [==============================] - 0s 205us/step - loss: 0.2257 - acc: 0.9372 - val_loss: 0.4204 - val_acc: 0.8845\n",
      "Epoch 210/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.2186 - acc: 0.9413 - val_loss: 0.4248 - val_acc: 0.8821\n",
      "Epoch 211/400\n",
      "1960/1960 [==============================] - 0s 213us/step - loss: 0.2068 - acc: 0.9413 - val_loss: 0.4272 - val_acc: 0.8738\n",
      "Epoch 212/400\n",
      "1960/1960 [==============================] - 1s 266us/step - loss: 0.2235 - acc: 0.9378 - val_loss: 0.4279 - val_acc: 0.8714\n",
      "Epoch 213/400\n",
      "1960/1960 [==============================] - 0s 242us/step - loss: 0.2215 - acc: 0.9332 - val_loss: 0.4322 - val_acc: 0.8702\n",
      "Epoch 214/400\n",
      "1960/1960 [==============================] - 1s 298us/step - loss: 0.1969 - acc: 0.9459 - val_loss: 0.4397 - val_acc: 0.8702\n",
      "Epoch 215/400\n",
      "1960/1960 [==============================] - 0s 253us/step - loss: 0.2116 - acc: 0.9459 - val_loss: 0.4475 - val_acc: 0.8714\n",
      "Epoch 216/400\n",
      "1960/1960 [==============================] - 1s 293us/step - loss: 0.2019 - acc: 0.9429 - val_loss: 0.4609 - val_acc: 0.8631\n",
      "Epoch 217/400\n",
      "1960/1960 [==============================] - 0s 205us/step - loss: 0.1998 - acc: 0.9459 - val_loss: 0.4796 - val_acc: 0.8524\n",
      "Epoch 218/400\n",
      "1960/1960 [==============================] - 1s 264us/step - loss: 0.2138 - acc: 0.9367 - val_loss: 0.4980 - val_acc: 0.8476\n",
      "Epoch 219/400\n",
      "1960/1960 [==============================] - 0s 246us/step - loss: 0.2142 - acc: 0.9378 - val_loss: 0.5043 - val_acc: 0.8440\n",
      "Epoch 220/400\n",
      "1960/1960 [==============================] - 1s 297us/step - loss: 0.2216 - acc: 0.9362 - val_loss: 0.5017 - val_acc: 0.8476\n",
      "Epoch 221/400\n",
      "1960/1960 [==============================] - 1s 273us/step - loss: 0.1990 - acc: 0.9423 - val_loss: 0.4968 - val_acc: 0.8464\n",
      "Epoch 222/400\n",
      "1960/1960 [==============================] - 1s 315us/step - loss: 0.2114 - acc: 0.9418 - val_loss: 0.4914 - val_acc: 0.8476\n",
      "Epoch 223/400\n",
      "1960/1960 [==============================] - 0s 240us/step - loss: 0.2096 - acc: 0.9398 - val_loss: 0.4888 - val_acc: 0.8500\n",
      "Epoch 224/400\n",
      "1960/1960 [==============================] - 1s 271us/step - loss: 0.1974 - acc: 0.9444 - val_loss: 0.4807 - val_acc: 0.8536\n",
      "Epoch 225/400\n",
      "1960/1960 [==============================] - 0s 224us/step - loss: 0.2121 - acc: 0.9418 - val_loss: 0.4717 - val_acc: 0.8583\n",
      "Epoch 226/400\n",
      "1960/1960 [==============================] - 1s 284us/step - loss: 0.1993 - acc: 0.9474 - val_loss: 0.4573 - val_acc: 0.8619\n",
      "Epoch 227/400\n",
      "1960/1960 [==============================] - 0s 251us/step - loss: 0.2042 - acc: 0.9469 - val_loss: 0.4432 - val_acc: 0.8655\n",
      "Epoch 228/400\n",
      "1960/1960 [==============================] - 1s 275us/step - loss: 0.1894 - acc: 0.9449 - val_loss: 0.4304 - val_acc: 0.8750\n",
      "Epoch 229/400\n",
      "1960/1960 [==============================] - 0s 228us/step - loss: 0.2084 - acc: 0.9418 - val_loss: 0.4227 - val_acc: 0.8750\n",
      "Epoch 230/400\n",
      "1960/1960 [==============================] - 1s 318us/step - loss: 0.2003 - acc: 0.9418 - val_loss: 0.4217 - val_acc: 0.8726\n",
      "Epoch 231/400\n",
      "1960/1960 [==============================] - 1s 323us/step - loss: 0.2040 - acc: 0.9439 - val_loss: 0.4232 - val_acc: 0.8714\n",
      "Epoch 232/400\n",
      "1960/1960 [==============================] - 1s 321us/step - loss: 0.1965 - acc: 0.9418 - val_loss: 0.4223 - val_acc: 0.8714\n",
      "Epoch 233/400\n",
      "1960/1960 [==============================] - 0s 253us/step - loss: 0.2144 - acc: 0.9423 - val_loss: 0.4255 - val_acc: 0.8714\n",
      "Epoch 234/400\n",
      "1960/1960 [==============================] - 1s 293us/step - loss: 0.2009 - acc: 0.9474 - val_loss: 0.4267 - val_acc: 0.8702\n",
      "Epoch 235/400\n",
      "1960/1960 [==============================] - 0s 237us/step - loss: 0.1962 - acc: 0.9418 - val_loss: 0.4301 - val_acc: 0.8726\n",
      "Epoch 236/400\n",
      "1960/1960 [==============================] - 1s 271us/step - loss: 0.1867 - acc: 0.9531 - val_loss: 0.4330 - val_acc: 0.8702\n",
      "Epoch 237/400\n",
      "1960/1960 [==============================] - 0s 237us/step - loss: 0.2016 - acc: 0.9464 - val_loss: 0.4307 - val_acc: 0.8690\n",
      "Epoch 238/400\n",
      "1960/1960 [==============================] - 1s 304us/step - loss: 0.2044 - acc: 0.9413 - val_loss: 0.4255 - val_acc: 0.8714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/400\n",
      "1960/1960 [==============================] - 0s 233us/step - loss: 0.2076 - acc: 0.9403 - val_loss: 0.4185 - val_acc: 0.8726\n",
      "Epoch 240/400\n",
      "1960/1960 [==============================] - 1s 282us/step - loss: 0.1936 - acc: 0.9454 - val_loss: 0.4111 - val_acc: 0.8762\n",
      "Epoch 241/400\n",
      "1960/1960 [==============================] - 0s 240us/step - loss: 0.1837 - acc: 0.9551 - val_loss: 0.4071 - val_acc: 0.8798\n",
      "Epoch 242/400\n",
      "1960/1960 [==============================] - 1s 301us/step - loss: 0.1971 - acc: 0.9480 - val_loss: 0.4038 - val_acc: 0.8857\n",
      "Epoch 243/400\n",
      "1960/1960 [==============================] - 0s 220us/step - loss: 0.1940 - acc: 0.9469 - val_loss: 0.3986 - val_acc: 0.8869\n",
      "Epoch 244/400\n",
      "1960/1960 [==============================] - 1s 284us/step - loss: 0.1981 - acc: 0.9480 - val_loss: 0.3940 - val_acc: 0.8857\n",
      "Epoch 245/400\n",
      "1960/1960 [==============================] - 0s 215us/step - loss: 0.1977 - acc: 0.9485 - val_loss: 0.3881 - val_acc: 0.8893\n",
      "Epoch 246/400\n",
      "1960/1960 [==============================] - 1s 293us/step - loss: 0.1813 - acc: 0.9444 - val_loss: 0.3817 - val_acc: 0.8929\n",
      "Epoch 247/400\n",
      "1960/1960 [==============================] - 0s 222us/step - loss: 0.2133 - acc: 0.9357 - val_loss: 0.3775 - val_acc: 0.8976\n",
      "Epoch 248/400\n",
      "1960/1960 [==============================] - 1s 291us/step - loss: 0.2050 - acc: 0.9444 - val_loss: 0.3743 - val_acc: 0.8964\n",
      "Epoch 249/400\n",
      "1960/1960 [==============================] - 0s 248us/step - loss: 0.1784 - acc: 0.9500 - val_loss: 0.3725 - val_acc: 0.8964\n",
      "Epoch 250/400\n",
      "1960/1960 [==============================] - 1s 295us/step - loss: 0.2048 - acc: 0.9459 - val_loss: 0.3712 - val_acc: 0.8976\n",
      "Epoch 251/400\n",
      "1960/1960 [==============================] - 0s 230us/step - loss: 0.1922 - acc: 0.9485 - val_loss: 0.3699 - val_acc: 0.8988\n",
      "Epoch 252/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.2011 - acc: 0.9469 - val_loss: 0.3679 - val_acc: 0.8988\n",
      "Epoch 253/400\n",
      "1960/1960 [==============================] - 0s 215us/step - loss: 0.2050 - acc: 0.9495 - val_loss: 0.3676 - val_acc: 0.9000\n",
      "Epoch 254/400\n",
      "1960/1960 [==============================] - 1s 282us/step - loss: 0.1749 - acc: 0.9556 - val_loss: 0.3673 - val_acc: 0.8976\n",
      "Epoch 255/400\n",
      "1960/1960 [==============================] - 0s 235us/step - loss: 0.1960 - acc: 0.9449 - val_loss: 0.3683 - val_acc: 0.8976\n",
      "Epoch 256/400\n",
      "1960/1960 [==============================] - 1s 313us/step - loss: 0.1932 - acc: 0.9495 - val_loss: 0.3687 - val_acc: 0.8964\n",
      "Epoch 257/400\n",
      "1960/1960 [==============================] - 1s 310us/step - loss: 0.1885 - acc: 0.9505 - val_loss: 0.3689 - val_acc: 0.8964\n",
      "Epoch 258/400\n",
      "1960/1960 [==============================] - 1s 406us/step - loss: 0.1976 - acc: 0.9500 - val_loss: 0.3702 - val_acc: 0.8952\n",
      "Epoch 259/400\n",
      "1960/1960 [==============================] - 0s 245us/step - loss: 0.1961 - acc: 0.9454 - val_loss: 0.3720 - val_acc: 0.8929\n",
      "Epoch 260/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.1878 - acc: 0.9454 - val_loss: 0.3734 - val_acc: 0.8940\n",
      "Epoch 261/400\n",
      "1960/1960 [==============================] - 0s 222us/step - loss: 0.1763 - acc: 0.9520 - val_loss: 0.3741 - val_acc: 0.8952\n",
      "Epoch 262/400\n",
      "1960/1960 [==============================] - 1s 325us/step - loss: 0.1829 - acc: 0.9546 - val_loss: 0.3741 - val_acc: 0.8952\n",
      "Epoch 263/400\n",
      "1960/1960 [==============================] - 0s 236us/step - loss: 0.1862 - acc: 0.9480 - val_loss: 0.3749 - val_acc: 0.8940\n",
      "Epoch 264/400\n",
      "1960/1960 [==============================] - 1s 270us/step - loss: 0.1787 - acc: 0.9515 - val_loss: 0.3760 - val_acc: 0.8929\n",
      "Epoch 265/400\n",
      "1960/1960 [==============================] - 0s 236us/step - loss: 0.1855 - acc: 0.9449 - val_loss: 0.3778 - val_acc: 0.8929\n",
      "Epoch 266/400\n",
      "1960/1960 [==============================] - 1s 286us/step - loss: 0.1803 - acc: 0.9526 - val_loss: 0.3799 - val_acc: 0.8929\n",
      "Epoch 267/400\n",
      "1960/1960 [==============================] - 0s 217us/step - loss: 0.1786 - acc: 0.9500 - val_loss: 0.3813 - val_acc: 0.8929\n",
      "Epoch 268/400\n",
      "1960/1960 [==============================] - 1s 295us/step - loss: 0.1864 - acc: 0.9480 - val_loss: 0.3833 - val_acc: 0.8940\n",
      "Epoch 269/400\n",
      "1960/1960 [==============================] - 0s 233us/step - loss: 0.1772 - acc: 0.9500 - val_loss: 0.3841 - val_acc: 0.8940\n",
      "Epoch 270/400\n",
      "1960/1960 [==============================] - 1s 328us/step - loss: 0.1703 - acc: 0.9526 - val_loss: 0.3843 - val_acc: 0.8940\n",
      "Epoch 271/400\n",
      "1960/1960 [==============================] - 0s 228us/step - loss: 0.1897 - acc: 0.9469 - val_loss: 0.3852 - val_acc: 0.8940\n",
      "Epoch 272/400\n",
      "1960/1960 [==============================] - 1s 304us/step - loss: 0.1786 - acc: 0.9531 - val_loss: 0.3868 - val_acc: 0.8929\n",
      "Epoch 273/400\n",
      "1960/1960 [==============================] - 0s 242us/step - loss: 0.1918 - acc: 0.9474 - val_loss: 0.3879 - val_acc: 0.8917\n",
      "Epoch 274/400\n",
      "1960/1960 [==============================] - 1s 331us/step - loss: 0.1695 - acc: 0.9582 - val_loss: 0.3893 - val_acc: 0.8905\n",
      "Epoch 275/400\n",
      "1960/1960 [==============================] - 0s 241us/step - loss: 0.1893 - acc: 0.9474 - val_loss: 0.3889 - val_acc: 0.8905\n",
      "Epoch 276/400\n",
      "1960/1960 [==============================] - 1s 307us/step - loss: 0.1799 - acc: 0.9505 - val_loss: 0.3875 - val_acc: 0.8905\n",
      "Epoch 277/400\n",
      "1960/1960 [==============================] - 0s 251us/step - loss: 0.1854 - acc: 0.9515 - val_loss: 0.3863 - val_acc: 0.8917\n",
      "Epoch 278/400\n",
      "1960/1960 [==============================] - 1s 337us/step - loss: 0.1782 - acc: 0.9490 - val_loss: 0.3854 - val_acc: 0.8905\n",
      "Epoch 279/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.1924 - acc: 0.9439 - val_loss: 0.3842 - val_acc: 0.8905\n",
      "Epoch 280/400\n",
      "1960/1960 [==============================] - 1s 343us/step - loss: 0.1809 - acc: 0.9454 - val_loss: 0.3828 - val_acc: 0.8917\n",
      "Epoch 281/400\n",
      "1960/1960 [==============================] - 0s 229us/step - loss: 0.1775 - acc: 0.9526 - val_loss: 0.3815 - val_acc: 0.8917\n",
      "Epoch 282/400\n",
      "1960/1960 [==============================] - 1s 329us/step - loss: 0.1676 - acc: 0.9546 - val_loss: 0.3801 - val_acc: 0.8893\n",
      "Epoch 283/400\n",
      "1960/1960 [==============================] - 1s 258us/step - loss: 0.1791 - acc: 0.9495 - val_loss: 0.3796 - val_acc: 0.8893\n",
      "Epoch 284/400\n",
      "1960/1960 [==============================] - 1s 309us/step - loss: 0.1706 - acc: 0.9485 - val_loss: 0.3788 - val_acc: 0.8917\n",
      "Epoch 285/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 0.1757 - acc: 0.9485 - val_loss: 0.3783 - val_acc: 0.8929\n",
      "Epoch 286/400\n",
      "1960/1960 [==============================] - 1s 276us/step - loss: 0.1721 - acc: 0.9551 - val_loss: 0.3781 - val_acc: 0.8929\n",
      "Epoch 287/400\n",
      "1960/1960 [==============================] - 1s 266us/step - loss: 0.1804 - acc: 0.9510 - val_loss: 0.3782 - val_acc: 0.8929\n",
      "Epoch 288/400\n",
      "1960/1960 [==============================] - 1s 307us/step - loss: 0.1891 - acc: 0.9480 - val_loss: 0.3780 - val_acc: 0.8917\n",
      "Epoch 289/400\n",
      "1960/1960 [==============================] - 1s 258us/step - loss: 0.1839 - acc: 0.9505 - val_loss: 0.3780 - val_acc: 0.8929\n",
      "Epoch 290/400\n",
      "1960/1960 [==============================] - 1s 303us/step - loss: 0.1780 - acc: 0.9531 - val_loss: 0.3780 - val_acc: 0.8917\n",
      "Epoch 291/400\n",
      "1960/1960 [==============================] - 0s 217us/step - loss: 0.1709 - acc: 0.9561 - val_loss: 0.3779 - val_acc: 0.8917\n",
      "Epoch 292/400\n",
      "1960/1960 [==============================] - 1s 276us/step - loss: 0.1887 - acc: 0.9454 - val_loss: 0.3782 - val_acc: 0.8929\n",
      "Epoch 293/400\n",
      "1960/1960 [==============================] - 0s 226us/step - loss: 0.1907 - acc: 0.9500 - val_loss: 0.3770 - val_acc: 0.8929\n",
      "Epoch 294/400\n",
      "1960/1960 [==============================] - 1s 319us/step - loss: 0.1838 - acc: 0.9490 - val_loss: 0.3753 - val_acc: 0.8940\n",
      "Epoch 295/400\n",
      "1960/1960 [==============================] - 0s 223us/step - loss: 0.1905 - acc: 0.9469 - val_loss: 0.3743 - val_acc: 0.8929\n",
      "Epoch 296/400\n",
      "1960/1960 [==============================] - 1s 309us/step - loss: 0.1789 - acc: 0.9505 - val_loss: 0.3733 - val_acc: 0.8988\n",
      "Epoch 297/400\n",
      "1960/1960 [==============================] - 0s 200us/step - loss: 0.1652 - acc: 0.9602 - val_loss: 0.3723 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298/400\n",
      "1960/1960 [==============================] - 1s 283us/step - loss: 0.1715 - acc: 0.9526 - val_loss: 0.3713 - val_acc: 0.9012\n",
      "Epoch 299/400\n",
      "1960/1960 [==============================] - 0s 222us/step - loss: 0.1726 - acc: 0.9561 - val_loss: 0.3705 - val_acc: 0.9024\n",
      "Epoch 300/400\n",
      "1960/1960 [==============================] - 1s 320us/step - loss: 0.1727 - acc: 0.9556 - val_loss: 0.3695 - val_acc: 0.9024\n",
      "Epoch 301/400\n",
      "1960/1960 [==============================] - 0s 202us/step - loss: 0.1777 - acc: 0.9485 - val_loss: 0.3685 - val_acc: 0.9036\n",
      "Epoch 302/400\n",
      "1960/1960 [==============================] - 1s 295us/step - loss: 0.1806 - acc: 0.9490 - val_loss: 0.3674 - val_acc: 0.9000\n",
      "Epoch 303/400\n",
      "1960/1960 [==============================] - 0s 240us/step - loss: 0.1704 - acc: 0.9520 - val_loss: 0.3661 - val_acc: 0.9000\n",
      "Epoch 304/400\n",
      "1960/1960 [==============================] - 1s 320us/step - loss: 0.1735 - acc: 0.9500 - val_loss: 0.3651 - val_acc: 0.9000\n",
      "Epoch 305/400\n",
      "1960/1960 [==============================] - 0s 216us/step - loss: 0.1694 - acc: 0.9520 - val_loss: 0.3644 - val_acc: 0.9024\n",
      "Epoch 306/400\n",
      "1960/1960 [==============================] - 1s 309us/step - loss: 0.1838 - acc: 0.9495 - val_loss: 0.3639 - val_acc: 0.9024\n",
      "Epoch 307/400\n",
      "1960/1960 [==============================] - 0s 223us/step - loss: 0.1844 - acc: 0.9485 - val_loss: 0.3633 - val_acc: 0.9012\n",
      "Epoch 308/400\n",
      "1960/1960 [==============================] - 1s 280us/step - loss: 0.1723 - acc: 0.9561 - val_loss: 0.3627 - val_acc: 0.9012\n",
      "Epoch 309/400\n",
      "1960/1960 [==============================] - 0s 229us/step - loss: 0.1798 - acc: 0.9485 - val_loss: 0.3623 - val_acc: 0.9012\n",
      "Epoch 310/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.1778 - acc: 0.9495 - val_loss: 0.3618 - val_acc: 0.9036\n",
      "Epoch 311/400\n",
      "1960/1960 [==============================] - 0s 214us/step - loss: 0.1834 - acc: 0.9439 - val_loss: 0.3613 - val_acc: 0.9048\n",
      "Epoch 312/400\n",
      "1960/1960 [==============================] - 1s 273us/step - loss: 0.1728 - acc: 0.9556 - val_loss: 0.3610 - val_acc: 0.9048\n",
      "Epoch 313/400\n",
      "1960/1960 [==============================] - 0s 199us/step - loss: 0.1706 - acc: 0.9551 - val_loss: 0.3607 - val_acc: 0.9048\n",
      "Epoch 314/400\n",
      "1960/1960 [==============================] - 1s 274us/step - loss: 0.1733 - acc: 0.9546 - val_loss: 0.3603 - val_acc: 0.9048\n",
      "Epoch 315/400\n",
      "1960/1960 [==============================] - 0s 223us/step - loss: 0.1667 - acc: 0.9536 - val_loss: 0.3602 - val_acc: 0.9048\n",
      "Epoch 316/400\n",
      "1960/1960 [==============================] - 1s 285us/step - loss: 0.1696 - acc: 0.9510 - val_loss: 0.3600 - val_acc: 0.9036\n",
      "Epoch 317/400\n",
      "1960/1960 [==============================] - 0s 231us/step - loss: 0.1690 - acc: 0.9541 - val_loss: 0.3599 - val_acc: 0.9036\n",
      "Epoch 318/400\n",
      "1960/1960 [==============================] - 1s 293us/step - loss: 0.1852 - acc: 0.9495 - val_loss: 0.3600 - val_acc: 0.9036\n",
      "Epoch 319/400\n",
      "1960/1960 [==============================] - 0s 204us/step - loss: 0.1820 - acc: 0.9469 - val_loss: 0.3601 - val_acc: 0.9036\n",
      "Epoch 320/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.1586 - acc: 0.9582 - val_loss: 0.3604 - val_acc: 0.9024\n",
      "Epoch 321/400\n",
      "1960/1960 [==============================] - 0s 232us/step - loss: 0.1722 - acc: 0.9536 - val_loss: 0.3608 - val_acc: 0.9048\n",
      "Epoch 322/400\n",
      "1960/1960 [==============================] - 1s 265us/step - loss: 0.1582 - acc: 0.9597 - val_loss: 0.3613 - val_acc: 0.9060\n",
      "Epoch 323/400\n",
      "1960/1960 [==============================] - 0s 236us/step - loss: 0.1968 - acc: 0.9474 - val_loss: 0.3618 - val_acc: 0.9060\n",
      "Epoch 324/400\n",
      "1960/1960 [==============================] - 1s 280us/step - loss: 0.1699 - acc: 0.9520 - val_loss: 0.3623 - val_acc: 0.9048\n",
      "Epoch 325/400\n",
      "1960/1960 [==============================] - 0s 228us/step - loss: 0.1655 - acc: 0.9582 - val_loss: 0.3626 - val_acc: 0.9048\n",
      "Epoch 326/400\n",
      "1960/1960 [==============================] - 1s 283us/step - loss: 0.1631 - acc: 0.9592 - val_loss: 0.3629 - val_acc: 0.9060\n",
      "Epoch 327/400\n",
      "1960/1960 [==============================] - 1s 259us/step - loss: 0.1579 - acc: 0.9526 - val_loss: 0.3631 - val_acc: 0.9060\n",
      "Epoch 328/400\n",
      "1960/1960 [==============================] - 1s 292us/step - loss: 0.1696 - acc: 0.9546 - val_loss: 0.3632 - val_acc: 0.9071\n",
      "Epoch 329/400\n",
      "1960/1960 [==============================] - 0s 226us/step - loss: 0.1586 - acc: 0.9587 - val_loss: 0.3635 - val_acc: 0.9048\n",
      "Epoch 330/400\n",
      "1960/1960 [==============================] - 1s 284us/step - loss: 0.1659 - acc: 0.9536 - val_loss: 0.3635 - val_acc: 0.9048\n",
      "Epoch 331/400\n",
      "1960/1960 [==============================] - 0s 243us/step - loss: 0.1717 - acc: 0.9515 - val_loss: 0.3635 - val_acc: 0.9048\n",
      "Epoch 332/400\n",
      "1960/1960 [==============================] - 1s 270us/step - loss: 0.1612 - acc: 0.9551 - val_loss: 0.3635 - val_acc: 0.9048\n",
      "Epoch 333/400\n",
      "1960/1960 [==============================] - 0s 231us/step - loss: 0.1837 - acc: 0.9495 - val_loss: 0.3631 - val_acc: 0.9048\n",
      "Epoch 334/400\n",
      "1960/1960 [==============================] - 1s 272us/step - loss: 0.1776 - acc: 0.9556 - val_loss: 0.3626 - val_acc: 0.9036\n",
      "Epoch 335/400\n",
      "1960/1960 [==============================] - 0s 226us/step - loss: 0.1740 - acc: 0.9459 - val_loss: 0.3621 - val_acc: 0.9048\n",
      "Epoch 336/400\n",
      "1960/1960 [==============================] - 1s 260us/step - loss: 0.1697 - acc: 0.9536 - val_loss: 0.3616 - val_acc: 0.9048\n",
      "Epoch 337/400\n",
      "1960/1960 [==============================] - 0s 218us/step - loss: 0.1683 - acc: 0.9592 - val_loss: 0.3612 - val_acc: 0.9060\n",
      "Epoch 338/400\n",
      "1960/1960 [==============================] - 1s 281us/step - loss: 0.1792 - acc: 0.9536 - val_loss: 0.3608 - val_acc: 0.9060\n",
      "Epoch 339/400\n",
      "1960/1960 [==============================] - 0s 226us/step - loss: 0.1814 - acc: 0.9500 - val_loss: 0.3605 - val_acc: 0.9060\n",
      "Epoch 340/400\n",
      "1960/1960 [==============================] - 1s 278us/step - loss: 0.1556 - acc: 0.9602 - val_loss: 0.3603 - val_acc: 0.9060\n",
      "Epoch 341/400\n",
      "1960/1960 [==============================] - 0s 227us/step - loss: 0.1688 - acc: 0.9561 - val_loss: 0.3603 - val_acc: 0.9060\n",
      "Epoch 342/400\n",
      "1960/1960 [==============================] - 1s 288us/step - loss: 0.1790 - acc: 0.9505 - val_loss: 0.3605 - val_acc: 0.9060\n",
      "Epoch 343/400\n",
      "1960/1960 [==============================] - 0s 216us/step - loss: 0.1794 - acc: 0.9500 - val_loss: 0.3605 - val_acc: 0.9060\n",
      "Epoch 344/400\n",
      "1960/1960 [==============================] - 1s 289us/step - loss: 0.1744 - acc: 0.9571 - val_loss: 0.3605 - val_acc: 0.9048\n",
      "Epoch 345/400\n",
      "1960/1960 [==============================] - 0s 248us/step - loss: 0.1696 - acc: 0.9526 - val_loss: 0.3605 - val_acc: 0.9036\n",
      "Epoch 346/400\n",
      "1960/1960 [==============================] - 1s 296us/step - loss: 0.1794 - acc: 0.9546 - val_loss: 0.3605 - val_acc: 0.9036\n",
      "Epoch 347/400\n",
      "1960/1960 [==============================] - 0s 210us/step - loss: 0.1614 - acc: 0.9546 - val_loss: 0.3605 - val_acc: 0.9048\n",
      "Epoch 348/400\n",
      "1960/1960 [==============================] - 1s 273us/step - loss: 0.1849 - acc: 0.9520 - val_loss: 0.3604 - val_acc: 0.9048\n",
      "Epoch 349/400\n",
      "1960/1960 [==============================] - 0s 228us/step - loss: 0.1689 - acc: 0.9520 - val_loss: 0.3604 - val_acc: 0.9048\n",
      "Epoch 350/400\n",
      "1960/1960 [==============================] - 1s 301us/step - loss: 0.1702 - acc: 0.9515 - val_loss: 0.3604 - val_acc: 0.9048\n",
      "Epoch 351/400\n",
      "1960/1960 [==============================] - 0s 238us/step - loss: 0.1663 - acc: 0.9490 - val_loss: 0.3606 - val_acc: 0.9048\n",
      "Epoch 352/400\n",
      "1960/1960 [==============================] - 1s 260us/step - loss: 0.1877 - acc: 0.9434 - val_loss: 0.3607 - val_acc: 0.9048\n",
      "Epoch 353/400\n",
      "1960/1960 [==============================] - 0s 214us/step - loss: 0.1694 - acc: 0.9515 - val_loss: 0.3608 - val_acc: 0.9048\n",
      "Epoch 354/400\n",
      "1960/1960 [==============================] - 1s 286us/step - loss: 0.1763 - acc: 0.9480 - val_loss: 0.3608 - val_acc: 0.9036\n",
      "Epoch 355/400\n",
      "1960/1960 [==============================] - 0s 223us/step - loss: 0.1777 - acc: 0.9546 - val_loss: 0.3607 - val_acc: 0.9036\n",
      "Epoch 356/400\n",
      "1960/1960 [==============================] - 1s 271us/step - loss: 0.1796 - acc: 0.9495 - val_loss: 0.3605 - val_acc: 0.9036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 357/400\n",
      "1960/1960 [==============================] - 0s 247us/step - loss: 0.1773 - acc: 0.9551 - val_loss: 0.3604 - val_acc: 0.9024\n",
      "Epoch 358/400\n",
      "1960/1960 [==============================] - 1s 319us/step - loss: 0.1775 - acc: 0.9551 - val_loss: 0.3603 - val_acc: 0.9036\n",
      "Epoch 359/400\n",
      "1960/1960 [==============================] - 1s 258us/step - loss: 0.1798 - acc: 0.9541 - val_loss: 0.3602 - val_acc: 0.9024\n",
      "Epoch 360/400\n",
      "1960/1960 [==============================] - 1s 281us/step - loss: 0.1554 - acc: 0.9592 - val_loss: 0.3600 - val_acc: 0.9036\n",
      "Epoch 361/400\n",
      "1960/1960 [==============================] - 0s 252us/step - loss: 0.1596 - acc: 0.9597 - val_loss: 0.3598 - val_acc: 0.9036\n",
      "Epoch 362/400\n",
      "1960/1960 [==============================] - 1s 272us/step - loss: 0.1738 - acc: 0.9515 - val_loss: 0.3595 - val_acc: 0.9036\n",
      "Epoch 363/400\n",
      "1960/1960 [==============================] - 0s 214us/step - loss: 0.1538 - acc: 0.9561 - val_loss: 0.3593 - val_acc: 0.9036\n",
      "Epoch 364/400\n",
      "1960/1960 [==============================] - 1s 290us/step - loss: 0.1831 - acc: 0.9464 - val_loss: 0.3591 - val_acc: 0.9048\n",
      "Epoch 365/400\n",
      "1960/1960 [==============================] - 0s 230us/step - loss: 0.1590 - acc: 0.9597 - val_loss: 0.3589 - val_acc: 0.9048\n",
      "Epoch 366/400\n",
      "1960/1960 [==============================] - 1s 284us/step - loss: 0.1847 - acc: 0.9485 - val_loss: 0.3588 - val_acc: 0.9048\n",
      "Epoch 367/400\n",
      "1960/1960 [==============================] - 1s 259us/step - loss: 0.1735 - acc: 0.9510 - val_loss: 0.3586 - val_acc: 0.9048\n",
      "Epoch 368/400\n",
      "1960/1960 [==============================] - 1s 298us/step - loss: 0.1772 - acc: 0.9526 - val_loss: 0.3585 - val_acc: 0.9048\n",
      "Epoch 369/400\n",
      "1960/1960 [==============================] - 0s 244us/step - loss: 0.1699 - acc: 0.9500 - val_loss: 0.3583 - val_acc: 0.9048\n",
      "Epoch 370/400\n",
      "1960/1960 [==============================] - 1s 288us/step - loss: 0.1757 - acc: 0.9546 - val_loss: 0.3581 - val_acc: 0.9048\n",
      "Epoch 371/400\n",
      "1960/1960 [==============================] - 0s 232us/step - loss: 0.1574 - acc: 0.9597 - val_loss: 0.3578 - val_acc: 0.9048\n",
      "Epoch 372/400\n",
      "1960/1960 [==============================] - 1s 273us/step - loss: 0.1952 - acc: 0.9429 - val_loss: 0.3576 - val_acc: 0.9036\n",
      "Epoch 373/400\n",
      "1960/1960 [==============================] - 0s 219us/step - loss: 0.1887 - acc: 0.9449 - val_loss: 0.3573 - val_acc: 0.9036\n",
      "Epoch 374/400\n",
      "1960/1960 [==============================] - 1s 295us/step - loss: 0.1662 - acc: 0.9536 - val_loss: 0.3570 - val_acc: 0.9036\n",
      "Epoch 375/400\n",
      "1960/1960 [==============================] - 0s 227us/step - loss: 0.1807 - acc: 0.9536 - val_loss: 0.3568 - val_acc: 0.9024\n",
      "Epoch 376/400\n",
      "1960/1960 [==============================] - 1s 289us/step - loss: 0.1729 - acc: 0.9556 - val_loss: 0.3566 - val_acc: 0.9024\n",
      "Epoch 377/400\n",
      "1960/1960 [==============================] - 1s 257us/step - loss: 0.1784 - acc: 0.9531 - val_loss: 0.3565 - val_acc: 0.9012\n",
      "Epoch 378/400\n",
      "1960/1960 [==============================] - 1s 323us/step - loss: 0.1606 - acc: 0.9597 - val_loss: 0.3565 - val_acc: 0.9024\n",
      "Epoch 379/400\n",
      "1960/1960 [==============================] - 0s 249us/step - loss: 0.1717 - acc: 0.9536 - val_loss: 0.3564 - val_acc: 0.9024\n",
      "Epoch 380/400\n",
      "1960/1960 [==============================] - 1s 285us/step - loss: 0.1664 - acc: 0.9526 - val_loss: 0.3563 - val_acc: 0.9024\n",
      "Epoch 381/400\n",
      "1960/1960 [==============================] - 0s 213us/step - loss: 0.1718 - acc: 0.9546 - val_loss: 0.3562 - val_acc: 0.9024\n",
      "Epoch 382/400\n",
      "1960/1960 [==============================] - 1s 265us/step - loss: 0.1528 - acc: 0.9612 - val_loss: 0.3560 - val_acc: 0.9024\n",
      "Epoch 383/400\n",
      "1960/1960 [==============================] - 0s 236us/step - loss: 0.1517 - acc: 0.9612 - val_loss: 0.3559 - val_acc: 0.9024\n",
      "Epoch 384/400\n",
      "1960/1960 [==============================] - 1s 283us/step - loss: 0.1585 - acc: 0.9551 - val_loss: 0.3557 - val_acc: 0.9024\n",
      "Epoch 385/400\n",
      "1960/1960 [==============================] - 0s 222us/step - loss: 0.1864 - acc: 0.9454 - val_loss: 0.3555 - val_acc: 0.9012\n",
      "Epoch 386/400\n",
      "1960/1960 [==============================] - 1s 269us/step - loss: 0.1637 - acc: 0.9561 - val_loss: 0.3553 - val_acc: 0.9012\n",
      "Epoch 387/400\n",
      "1960/1960 [==============================] - 0s 243us/step - loss: 0.1830 - acc: 0.9510 - val_loss: 0.3552 - val_acc: 0.9012\n",
      "Epoch 388/400\n",
      "1960/1960 [==============================] - 1s 299us/step - loss: 0.1676 - acc: 0.9520 - val_loss: 0.3550 - val_acc: 0.9012\n",
      "Epoch 389/400\n",
      "1960/1960 [==============================] - 0s 236us/step - loss: 0.1875 - acc: 0.9500 - val_loss: 0.3549 - val_acc: 0.9000\n",
      "Epoch 390/400\n",
      "1960/1960 [==============================] - 1s 279us/step - loss: 0.1566 - acc: 0.9592 - val_loss: 0.3548 - val_acc: 0.9000\n",
      "Epoch 391/400\n",
      "1960/1960 [==============================] - 0s 254us/step - loss: 0.1635 - acc: 0.9622 - val_loss: 0.3547 - val_acc: 0.9000\n",
      "Epoch 392/400\n",
      "1960/1960 [==============================] - 1s 299us/step - loss: 0.1630 - acc: 0.9551 - val_loss: 0.3547 - val_acc: 0.9000\n",
      "Epoch 393/400\n",
      "1960/1960 [==============================] - 0s 221us/step - loss: 0.1659 - acc: 0.9582 - val_loss: 0.3547 - val_acc: 0.9012\n",
      "Epoch 394/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.1554 - acc: 0.9577 - val_loss: 0.3547 - val_acc: 0.9012\n",
      "Epoch 395/400\n",
      "1960/1960 [==============================] - 0s 215us/step - loss: 0.1629 - acc: 0.9541 - val_loss: 0.3547 - val_acc: 0.9012\n",
      "Epoch 396/400\n",
      "1960/1960 [==============================] - 1s 276us/step - loss: 0.1747 - acc: 0.9531 - val_loss: 0.3547 - val_acc: 0.9012\n",
      "Epoch 397/400\n",
      "1960/1960 [==============================] - 0s 199us/step - loss: 0.1686 - acc: 0.9561 - val_loss: 0.3547 - val_acc: 0.9012\n",
      "Epoch 398/400\n",
      "1960/1960 [==============================] - 1s 287us/step - loss: 0.1746 - acc: 0.9500 - val_loss: 0.3547 - val_acc: 0.9012\n",
      "Epoch 399/400\n",
      "1960/1960 [==============================] - 0s 227us/step - loss: 0.1558 - acc: 0.9592 - val_loss: 0.3548 - val_acc: 0.9012\n",
      "Epoch 400/400\n",
      "1960/1960 [==============================] - 1s 384us/step - loss: 0.1776 - acc: 0.9520 - val_loss: 0.3548 - val_acc: 0.9012\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.8, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_0,X_1],Y,\n",
    "                    batch_size=len(Y),\n",
    "                    epochs=400,\n",
    "                    verbose=True,\n",
    "                    shuffle=True,\n",
    "                    callbacks=[lrScheduler],\n",
    "                    validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DD_Net.save_weights('weights/fine_lite.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With frame_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    print('epoch{}'.format(e))\n",
    "    X_0 = []\n",
    "    X_1 = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in tqdm(range(len(Train['pose']))): \n",
    "    \n",
    "        label = np.zeros(C.clc_fine)\n",
    "        label[Train['fine_label'][i]-1] = 1 \n",
    "        \n",
    "        p = np.copy(Train['pose'][i]).reshape([-1,22,3])\n",
    "        p = sampling_frame(p,C)\n",
    "        \n",
    "        p = normlize_range(p)\n",
    "        M = get_CG(p,C)\n",
    "        \n",
    "        X_0.append(M)\n",
    "        X_1.append(p)\n",
    "        Y.append(label)\n",
    "\n",
    "    X_0 = np.stack(X_0)  \n",
    "    X_1 = np.stack(X_1) \n",
    "    Y = np.stack(Y)\n",
    "   \n",
    "\n",
    "    DD_Net.fit([X_0,X_1],Y,\n",
    "            batch_size=len(Y),\n",
    "            epochs=1,\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])\n",
    "cnf_matrix = confusion_matrix(np.argmax(Y_test,axis=1),np.argmax(Y_pred,axis=1))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cnf_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
